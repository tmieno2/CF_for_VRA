---
title: "Response to Editor"
output:
  bookdown::word_document2:
    number_sections: no
    reference_docx: ../word_template_format.docx
bibliography: ../ML_VRA.bib
---

```{r echo = F, include = FALSE, cache = F}
library(knitr)
library(here)
here::i_am("GitControlled/Writing/CEA_3rd_round/replies_rev.rmd")
# opts_knit$set(root.dir = here())

options(htmltools.dir.version = FALSE)
options(knitr.duplicate.label = "allow")

opts_chunk$set(
  fig.align = "center",
  fig.retina = 5,
  warning = F,
  message = F,
  cache = T,
  echo = F
)

library(data.table)
library(ggplot2)
```

Thank you for your comments. In response to them, we made significant changes that greatly clarified our explanation in section 4.2 of why predicting yield levels well at the observed points need not imply accurate prediction of EONRs. Below, please find your comments in black and our replies in blue.

<!-- We really appreciate your comments, and we are glad that you expressed your own concerns despite the fact the reviewers were okay with the previous version. This round of revision turned out to be much more important than the previous one. One of your comments was particularity important, and we were able to correct our mistake before this paper is published. Thank you so much for making this article better together with us. We actually made significant changes to section 4.2, which I believe is much improved because it explains clearly why predicting yield levels well at the observed points do not necessarily mean predicting EONR well. Below, you find your comments in black and our replies in blue. -->


<br>

**1. The CNN result seems to be rather constant than linear. Please include a chart showing the CNN performance per training epoch to allow the reader to assess how good the CNN model converged.**

<span style="color:blue"> 
Please see Figure D.1 in Appendix D, which shows the convergence of CNN for a single simulation round. We cannot show this kind of figure for all the simulation rounds, but any interested readers can replicate our work as the codes will be publicly accessible. Please also see the figure below. CNN severely underestimates the impact of N and estimated yield response to N is almost flat, but not exactly flat. 
</span>

<!-- Please see the figure below showing some examples of yield response function estimated by CNN under "aabbyytt" modeling scenario. Each of the lines indicates yield response curve for different sites.
 -->

```{r, fig.width=7, fig.cap = "An illustrative example of yield response functions estimated by CNN (scenario: aabbyytt)"}
cnn_simRes_temp <- readRDS(here("Shared/Results/ex_cnn_y_ResCurve.rmd"))

ggplot(cnn_simRes_temp, aes(x = rate, y = pred_yield, colour = factor(unique_subplot_id))) +
  geom_point(size = 0.7) +
  geom_smooth(method = lm, se = FALSE, size = 0.3) +
  facet_wrap(~Model, ncol = 2) +
  theme(
    legend.title = element_blank(),
    legend.position = "none"
  ) +
  xlab("N (kg/ha)") +
  ylab("Predicted Yield (kg/ha)")
```

<br>

**2. Line 70. Why can't EOIRs be estimated by the intervention studies mentioned in 28ff?**

<span style="color:blue">  
We are not saying that the studies mentioned in 28ff did not estimate EOIRs. However, "true" EONRs on real fields and sites cannot be known with certainty, but can only be estimated. This principle is at the core of statistical analysis. We never observe the underlying laws that govern how the world works but we can "estimate" those processes. This means that when we try to evaluate how different models perform in predicting EONR using real data, we simply do not have a valid benchmark to compare against. Some may ask, then why don't we use models' yield prediction ability because yield is observable? And this is precisely the dangerous practice that we show can be harmful.

Please note that this is a stark contrast to "yield" (not EOIR) prediction because yields ARE observable and models can be verified. However, that is not the case for EOIR. This is why MC simulations are needed for which the underlying data generating process (how yield responds to N, and how field characteristics affect yield response to N) is "known" to the researcher. This way, we can test a model's ability to predict EONR, not yield. This approach is commonly used in statistical journals when new statistical methods are proposed. This is because we can never test the statistical property of a statistical method in principle using the real data. For example, please see @Wager2018a and @oprescu2019orthogonal.
</span>

<br>

**3. Line 194 simualtions**

<span style="color:blue">  
The typo was corrected.
</span>

<br>

**4. Line 194 How exactly were the simulation parameters (table c.1, figure c.1) inspired by the field study (line 161ff) ?**

<span style="color:blue">  
Some of the authors have significant experience designing, conducting and analyzing hundreds of on-farm precision experiments. The general results of those experiments provided plausible ranges of the optimal EONR and error size. 
</span>

<br>

**5. Line 321 It seems RSME was never defined.**

<span style="color:blue">  
Please see page 19, Eq. (7) and Eq. (8) for the precise definitions of the RMSE for yield and EONR predictions.
</span>

<br>

**6. In principle, your ML models have to learn to replicate equation 3, right? You have to explain better why RSME for yield and EONR can be uncorrelated when they are directly related through Eq. 3 and 5. There must be an effect of yield prediction error on EONR. By definition, a 100% perfect yield prediction should reduce the EONR error to the effect of epsilon in Eq. 3. (As a side note, in this case the correlation between the RSMEs would also be zero.) A larger RSME for yield prediction would simultaneously increase the RSME of EONR until the EONR virtually becomes a random variable. Your comparison in Figure 6 may not be insightful in this regard. It might be confounded by effects from different prediction performance for different simulation parameters i.e. the prediction performance might be different depending on where you are in the hyperbola from Eq.3 . If the prediction performance is not uniform or the prediction error is of a similar size as the achievable yield increase, then it is natural that the RSMEs of EONR and yield prediction are not correlated. Figure C1.4 indicates that the effect size of the random error epsilon alone is already in the range of the achievable yield increase. I recommend to add a simulation of different ML performances and different epsilon values to test their effects.**

<span style="color:blue">  
We agree that the results presented in Figure 6 of the previous manuscript were invalid and basically compared apples and oranges. The figure was mixing RMSEs from all the simulation rounds and finding the $R^2$ of the two relationships. That is why $R^2$ was extremely low. Therefore, we decided to remove the figure entirely. Also, we no longer make the claim that yield and EONR predictions are completely uncorrelated. Thank you for the heads-up. Figure 8 of the revised manuscript plots RMSE of yield and EONR predictions by model and simulation round for several simulation rounds (simulation round 10, 20, and 30). 

In addition, we followed your suggestion to conduct simulations with different degrees of error. **The results are shown in Appendix G**. Examination of Tables 1, 2, G.1, and G.2 reveals that as the error size decreases, both yield and EONR RMSEs decrease. So, in this sense, yield and EONR predictions were indeed correlated, as expected.

We maintain in the revised manuscript our claim that the models with the highest yield prediction power need not provide the best (or the most profitable) EONR prediction. Please note that this claim comes from comparisons **across the models** given the **same** dataset (so the same errors). Since we consider this a very important point for researchers in this field, in the revised manuscript's section 4.2 we provide a more in-depth discussion. As you suggested, models that can recover the "structure" of the yield response function will perform well in predicting EONR. However, yield levels can sometimes be predicted well but for "wrong reasons," making it possible to predict yield levels at the observed rates (rates that are actually applied in the experiment) without recovering the structure of the yield response function. The training process of prediction-oriented ML methods may twist the reasoning behind why yield varies, and do so in a way that sacrifices its ability to recover the yield response function as long as it fits the primary objective of predicting yield levels well at the "observed" nitrogen rate for each site. Please see our more detailed discussions on this matter in section 4.2.
</span>

<br>

**7. Please insert a plot for true yield vs yield prediction to enable the reader to estimate the achieved prediction performance.**

<span style="color:blue">  
Please see Figure E.1, which plots true yields (without error, epsilon) against predicted yields by model for a single simulation round. (CF is not presented as it does not predict yield levels, as discussed in the article.) RMSE is presented in the figure as well. As we have 1000 simulations, it is infeasible to present figures like this for all the simulation rounds. But, this figure provides a better sense of the value of RMSE and the goodness of fit. 
</span>

<br>

**8. Please show to which degree N affected yield and how the relation between yield estimation error and the effect of N on yield increase was. I would like to see whether the models were able to reproduce the effect of N on yield at all.**

<span style="color:blue">  
Please see Figure 2 for illustrations of how yield responds to N site-specifically for several sites within a field. Figure 6 illustrates exactly the models' ability to predict the impact of N on yield at different treatments (lowest to the second lowest, second lowest to third lowest, etc). The figure shows that RF dramatically underestimates the impact of N on yield, which leads to the underestimation of EONR. BRF suffers negative bias as well. CF does not suffer this bias. Of course, CNN is the worst, which thinks that the impact of N on yield is almost nothing<!-- nascent --> as you can see in Figure 6 in the revised manuscript and Figure 1 in this document<!-- Figure 5 -->. In addition to the negative biases from RF and BRF, their estimation of the impact of N is less accurate than CF. That is why CF provides higher profits than do RF and BRF. CF does not even estimate Eq. (3), but rather estimates the change in yield when N is increased discretely, as is discussed in the revised manuscript. 
</span>

<br>

**9. Please extend the introduction by restating numbers for the state of the art of yield and EONR prediction performance from previous studies (as far as such numbers are available) to prevent that all of your readers have to consult the cited references.**

<span style="color:blue">
Please see page 3, line 35 for a newly listed study (@barbosa2020risk)  that attempts to estimate EONR. Please note that the previous studies relevant to our study apply ML methods to OFE data with the ultimate intention of estimating site-specific EONR. There is no study that tests the performance of EONR using real data, which is fundamentally impossible, as mentioned above. There are studies that predict field-level (but not site-specific within a field) EONRs for individual fields. Some use a traditional regression approach then model EONR using ML methods to see how close their estimated field-level EONRs from the trained ML methods are to the EONR "estimated" for individual fields (e.g., @Qin2018; @Ransom2019 @Wang2021; @wen2021machine). But, please note that the field-level EONRs they compare against are "estimated." Of course, they had to "estimate" the EONR for each field because they did not (and could not) know the true EONR for each field. Moreover, the objective of such studies are fundamentally different from ours. Our focus is on site-specific EONR within a field, theirs are on field-specific EONR. We consider those studies not very relevant to our purposes, and have left discussion of them out of the introduction for brevity.
</span>

<br>

**10. Please discuss your results with regard to the state of the art (again as specific as possible) and discuss whether your yield prediction performance and your simulation approach (e.g. epsilon effect size) may have affected the results for EONR prediction.**

<span style="color:blue">  
We discussed our results with regard to the state of the art literature. (Please see lines 352-357, and lines 442-475.) The state of the art is sadly trying to predict yields site-specifically as well as possible. We show that such practice can be misleading and economically harmful when the ultimate goal is to help farmers make more profitable nitrogen application decisions (predicting EONR well). There is nothing wrong with our approach in yield prediction as evident in Figure E.1<!-- Figure 12 -->. We are using ML methods that have been used in the on-farm experimentation literature for site-specific input management. Those studies were published very recently. We added our description about how the size of epsilon affects our results briefly in Appendix G. We added the following at line 381 page 24:

"*To check for model robustness, simulations were also run assuming smaller and larger error term sizes. Results presented in Appendix G show qualitative results basically unchanged.*"

</span>

<br>


# References

<div id="refs"></div>