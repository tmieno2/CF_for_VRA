---
title: "Machine Learning Methods for Site-specific Input Management"
author:
  - Shunkei Kakimoto^[University of Nebraska Lincoln, skakimoto3@hunskers.unl.edu] and Taro Mieno^[University of Nebraska Lincoln, tmieno2@unl.edu] 
output:
  bookdown::word_document2:
    toc: FALSE
    pandoc_args: ["-Fpandoc-crossref"]
bibliography: MSthesis.bib
eqnPrefix: 
      - "Eq"
abstract: "Abstract: Site-specific crop yield response to inputs is a complex biological process involving numerous factors and is heterogeneous within a field. Past studies have attempted to estimate optimal Nitrogen(N) fertilizer rates using various prediction-oriented  Machine Learning(ML) methods to forcast more accurate N recommendation rate. However, the problem we should be dealing with for the recommendation is to estimate heterogeneous causal impact of N fertilizer on crop yield. In other words, estimating site-specific optimal input use should be fundamentally based on a causal question. In this study, we apply Causal Forest(CF), which was recently developed by @Wager2018a and @athey2016recursive for estimating heterogeneous treatment effects, to the identification of site-specific optimal N fertilizer use. We evaluate its performance comparing with Random forest(RF) and Boosted Random Forest(BRF) under various model configurations. Futerhmore, we introduce spatial smoothing to generalize the spatial trend of the estimated site-specific optimal N rates. The data to be used for the analysis is generated with Monte Calro simulation incorporating geostatistical model. Results show that CF outperforms RF and BRF and spatial smoothing boost the accuracy of the recomendation use in all the cases. Thus we conclude that CF is a promising optimal N rate recommendation tool taking place of prediction-oriented ML methods such as RF and BRF, and spatial smoothing should be used together with it to derive more accurate estimates"
---

```{r echo = F, results='hide', message=FALSE}
library(knitr)
library(here)

# === set wd ===#
# Notes: not really necessary to set the working directory as long as this Rmd file is part of an R project and the evaluation directory was set to "Project".

opts_knit$set(root.dir = here())
# setwd(here())

options(htmltools.dir.version = FALSE)
options(knitr.duplicate.label = "allow")

opts_chunk$set(
  fig.align = "center", 
  fig.retina = 5,
  warning = F, 
  message = F,
  cache = FALSE, # <-- T
  cache.lazy = FALSE,
  echo = F
  )
```

```{r, cache = TRUE}
# === packages ===#
library(sf)
library(data.table)
library(grf)
library(RColorBrewer)
library(patchwork)
library(plotly)
library(magrittr)
library(ggplot2)
library(viridis)
library(tmap)
library(tidyverse)
library(ggthemes)
library(mgcv)
library(ggpubr)
library(flextable)
library(officer)
library(modelsummary)
library(raster)
library(bookdown)
```

```{r analyses, results = "hide", fig.show='hide', cache = FALSE}
#/*=================================================*/
#' # Preparation
#/*=================================================*/
# This is where you prepare datasets and results 

#/*----------------------------------*/
#' ## Create figures and tablesb
#/*----------------------------------*/
# --- Convert an Rmd file into a pure R file using purl() ---#
# knitr::purl(here("Codes/PrepareResults.Rmd"), output = here("Codes/PrepareResults.R"))

# --- Read the simulation results and prepare figures and tables ---#
source(here("Codes", "PrepareResults.R"))
```

# Introduction {-}
<!-- 
estimation of yield response to an input
"big data" 
heterogeneity of yeild response; different marginal yield response to an additional unit of input.
"yield heterogeneity identified with yield maps from combine harvesters cannot serve as a robust indicator for the profitability of site-specific fertilizer management"
-->

<!-- + background on nitrogen management (economics, environment, precision agriculture)   -->

Modern agricultural production system, which is featured by the enlargement of fields due to capital-labour substitution, has been confronting the difficulty in taking account of the spatial and temporal within-field heterogeneity of crop <!-- and environmental taits into its --> mangement[@zhang2002precision]. Precision agriculture(PA) is the technology developed to assist  decision-making for such a complicated management <!-- of soil and crops in a field  -->utilizing the power of information technology[@cook1998precision; @finger2019precision]. One of the leading PA technologies is variable rate application(VRA) and it has been actively investigated for site-specific input managment especially for nitrogen(N) management[@Khosla2002; @ruffo2006site; @koch2004economic; @hurley2004estimating; @shanahan2008responsive].
Effiecient managment of N fertilizer is a key component of crop production from agromical, economical and environmental standpoints.<!--  N fertilization is usually conducted to assure to meet crop N demand but --> 
Inappropriate amount or timing of N application adversely affects both biomass yields and grain quality for some crops. Furthermore, inefficient use of N fertilizer not only results in economic loss but causes ambient environmental losses in such a form of nitrate leaching[@blumenthal2008importance; @dinnes2002nitrogen; @tremblay2006adapting]. \
<!-- Excess amount of N fertilizer application could cause low nitrogen-use-efficiency, resulting in economic loss and ambient environmental losses in such a form of nitrate leaching[@]. -->

While site-specific input management has been rising in importance as a management tool to derive maximum economic gains and reduce environmental losses, there still remains formidable challenges we should deal with until it becomes widely accepted among farmers. One of the biggest challenges lies in how to identify the heterogeneous yield responses to inputs based on measurable field and soil properties <!-- and dynamic facotrs such as weather conditions -->. Despite considerable effort for modeling yield response to N in the past decades, there is no general agreement on its specification to date. Earlier studies modeling yeild response functions with traditional econometric approaches[@morris2018strengths]. However, those approaches may not be approptiate to capture the complex biological process between crop yield and inputs because the process involves numerous number of factors and they are likely to be interacted in non-linear and multi-way fashion[@Miao2006; @kitchen2003soil]. 
Critically, the ability of estimating heterogeneous imapct of inputs on crop yield, which is essential for site-specific input recommendation, is ristriced in the current econometric approaches[@Storm2020].
As an alternative approach, machine learning(ML) based site-specific input use recommendation has been gathering much attention in accompanying with the availability of sufficient spatially dense data related crop and field properties based on on-farm experiments. ML method has an advantage <!-- over traditional econometric models[cite papers]  -->in handling a large number of variables which are highly interacted in a nonlinear fashion and allow modeling in a data-driven way. Under little prior knowlege about underlying yield response function, ML method is appealing to describe complex plant and soil process. Indeed, some study reviewed application of ML methods in the literature of PA have shown the promissing usefulness of ML methods as a tool for yield prediction and Nitrogen management[@Chlingaryan2018;@van2020crop].\
<!-- + the gap -->
<!-- causal analysis for site-specific input use recommendation -->
However, there is currently a scarcity of literature applied ML methods to a "within-field" site-specific input management. 
Recent on-firm experiments enables farmers to collect more spatially dense data <!-- related to underying yield response function at finer scale level --> at low cost[@bullock2020value]. According to such data availability, ML methods should be evaluated for its potential performance as a within-field site-specific N reccomendation tool. On the other hand, while ML methods are promissing, previous studies commonly showed the difficulty in estimaing optimal N rates[@Qin2018; @Wang2021; @coulibali2020]. The potential mistake those studies made is that they depend on prediction-oriented ML methods to estimate site-specific optimal N rate.<!--  For example, @Qin2018 and @Ransom2019 used various ML methods to directly predict optimal N rate. @Wang2021 and @coulibali2020 attempted to found an optimal N rate based on yield prediction. However, --> However, for site-specific input mangement, the fundamental problem we should be asking is not prediction accuracy but to correctly estimate heterogeneous causal impact of inputs. This is because an optimal input use rate for a certain site heavily depends on the small change in yield casued by change in N rate conditional on other soil and field properties for that site[@puntel2016modeling].<!--  Thus, estimation of site-specific optimal N rate is a causal reserch question. --> As far as we know, no studies applied causal anlysis with ML methods in the context of site-specific input management. 
Another thing we want to highlight is that the resulting optimal N rate from ML methods does not accout for spatial structure. In the real world, any yield monitor observation <!-- crop and field characteristics --> has spatially smooth nature[@anselin2004spatial; @lambert2004comparison]. Thus, the optimal N rate which determined by those factors also should have spatially continuous feature across all locations in a field. This property has been overlooked in the past literature. 
<!-- + objective of the paper (how it fills the important research gap you mentioned above) -->
To fill the aforementioned research gaps, in this study we want to propose an alternative approach with a ML method for a site-specific input management. Our approach consists of two parts. 
First, we estimate causal impact of an input with with a relatively newly introduced ML method called Causal Forest(CF). CF is a ML method developed by @Wager2018a and @athey2016recursive for the estimation of heterogeneous treatment effects. We use CF to identify site-specific input use rates.\
<!-- Although the building block of CF is random forest[@breiman2001random], CF is essentialy distinct from prediction oriented ML methods in that its algorithm is optimized for identifying heterogeneous causal effects of a treatment. Indeed, in economics fields, many studies has started to adopted CF for causation research instead of using traditional econometric tools.  -->
<!-- If CF could successfuly cpature the causal impact of an input based on site-specific field and soil properties and we could utilize the information of the estimated causal impact for site-specific input use recommendation rate, we could obtain more reliable and precise site-specific recommendation input use rate.  -->
Second, spatilly smoothing is induced to the estimated optimal N rate based on the other field and soil properties and the geographical locations with generalized additive model(GAM)[@hastie1990generalized].　GAM has an advantage in nonparametrically capturing the pattern of the dependent variable <!-- using an appropriate continuous function for each independent variables --> in a data-driven way and producing good prediction[@yee1991generalized; @plant2018spatial]. <!-- We fit GAM to the estimated optimal N rates resulted from the CF using the other covariates as its independent variabls, and then re-estimate the spatially smoothed optimal N rates. --> We hypothesize that a series of these procedures produces more accurate N recommendation rate than approches using conventional prediction-oriented ML methods. 
<!--  -->
The objectives of this study are therefore: (1)to evaluate the relative performance of CF for site-specifc N rate recommendation  compared to random forest and boosted random forest which are widely accepted prediction-oriented ML methods in the past studies, and (2)to evaluate if spatial smoothing with GAM could be helpful to modify the estimates of optimal N rate.\
<!-- + how do you do it (summarize, do not provide too much details) -->
We test the performance of each of the ML methods and spatial smoothing through Monte Calro simulation. One thousand roudns of siumulation is conducted in total to generate the dataset of production characteristics for the simulated fied. In data simulation process, we make some variations in covariates to be used for analysis to reflect realistic probelems we face in practice. <!-- For each simulation round, each of the ML methods are used to estimate site-specific optimal N rate and spatial smoothing is applied to it. --> <!-- R-squared($R^2$) statistic is used as a metric for the evaluation of optimal N rate resulted from both ML methods and spatial smoothing each round, then the mean of $R^2$ values are calcuated for the final result. -->\
<!-- + summarize the results -->
The results show that that the approach using CF outperformed RF and BRF in estimating site-specific optimal N rate for all the model configurations. In addition, spatial smoothing considerably improved the precision of the recommendation rate. Even after GAM is applied, the results from CF still show the best performance. Hence, we conclude that CF is a promissing ML method as a site-specific input use recommendation tool, and spatial smoothing with GAM can aid the precision of the estimation. 

<!-- Flow
The reminder of this paper is organized as follows. In section 2, we review.... . Section 3 reviews.... In section 4...  -->


# Literature Review {-}
<!-- 
Think why do you need to do literature review
• to find research gap
- research gap1: 
- there is a problem in their methodology (they rely on prediction base approach)
- I want to say that ignoring to estimate yield response function in obtaining EONR is problematic;
- -> @Lory2003; 
- direct optimal N prediction fails at capturing the true optimal N rate.
-->
<!-- 
we want to propose a better approach for site-specific optimal N recommendation. 
 -->

<!-- Previous studies used ML methods mainly to directly predict optimal nitrogen recommedation rate. 

but the results suggested that those infomation did not contribute to the explanation of such the valiability.

@Qin2018 used various ML methods to predicted optimal nitrogen application rate using the on-farm experiments of 47 field-years in the midwestern US. Inspite of a large set of input data related to field-level soil and historical weather data, their results suffered from high prediction error. They also found that the prediction accuracy of ML did not change between using real weather data and using weather estimates devived from the historical weather trend.

Similarly, @Ransom2019 evaluated multiple ML methods for corn nitrogen recommendation incorporating soil and weather condition. Their comparison of the perfomance of ML methods using full infomation and the reduced infomation suggested that including more input variables did not improve the perfomance of ML methods.

@lawes2019 attempted to identify which variables are useful for optimal nirtogen prediction with random forest using simulated nitrogen response trial dataset. They found that historical site mean yield of the field is the most important variable rather than variables of soil type and subsoil conditions. Further, the prediction perfomance of random forest with different sets of independent variables did not change significantly, and their predictions were consistently underestimated. 

Meanwhile, @Wang2021 evaluates the perfomance of random forest for recommendation of optimal side-dress N rate. 

Even though the model can predict yield with high accuracy, when it comes to the estimation of optimal N rate their model fails to explain the variability of true optimal N. Similar results are confirmed in @puntel2016modeling and @puntel2018systems, although they did not use ML methods for predicting yield. -->




<!-- draft -->
Previoius studies examined many prediction-oriented ML methods in an attempt to evaluate their potential usefulness as a decision tool for optimal N recommendation. 
@Qin2018 used various ML methods such as random forest, gradient boost regression trees forests and ridge regression to esitmate optimal N application rate for corn fields across midwestern US. However, their results suffered from high prediction error despite using a large set of input data including soil properties and historical weather data, indicating the difficualty in predicting optimal N rate directly. 
@Ransom2019 examined how ML methods can be helpful for improving N recommendation rate that is derived from existing N recommendation methods<!--  using soil and weather information -->. Although they found that ML methods especially random forest performed better at adjusting exisiting N recommendation tools significnatly, their approaches are not suitable for within-field site-specific N managment because exisiting N recommendation tools are desined for regional-scale or field-scale. Meanwhile, @Wang2021 attempted to estimate optimal side-dress N application rate for corn at infiel-scale with random forest and Lasso linear regression. Different from the approach taken by @Qin2018, they first estimated yield response function with random forest using the data of soil properties, weather condition, crop management practicies and remote sensing data, then estimated optimal N rate by finding the profit-maximizing N rate. They found that the random forest shows reasonable performance in yield prediction, but the accuracy of the resulting optimal N recommentation rates are relatively poor, suggesting that the model that has high ability for yield prediction does not necessary leads to accurate estimates of optimal N rate. Similar results are also reported in @coulibali2020 and other studies employing a different approach other than ML methods. @puntel2016modeling and @puntel2018systems used a crop simulation model to predict yield and derived the optimal N rate. They achieved high accuracy in predicting yield but had a difficulty in estimating optimal N rate. Regarding this finding, they concluded that estimating optimal N rate is highly sensitive to the small changes in yield as N varies and so, it is more complicated than predicting yields.

* What are the problems with these studies? Why are they not useful? 

while they suggested the potential usefulness of ML methods as a decision makeing tool for N management compared to already prevailing N recommendation tools, their results also commonly implied that there is some limitation in estimating optimal N rate with prediction base ML methods<!--  even though the model is built with a large set of varibles which are supposed to explain the underlyign yield response functions -->. It is worthwhiel to seek an alternatinve approach with ML methods to make more reliable recommendations which is necessary to make site-specific optimal input management profitable.
<!-- there is only a limited number of studies forcused on the application of  ML methods for within-field site-specific optimal N management.  -->
The critical problem we can see in those past studies is that they attemped to use prediction-oriented ML methods for estimating yield response to N rate. They disregard an important fact; to estimate how site-specific yield changes to an additional unit of N application rate conditional on other factors.
<!-- apply an optimal amount of an input to be applied for each site to gain maximum net economic returns.  -->
<!-- For the goal, the problem the we need to deal with is to understand how the marginal net benefit of N application varies with site-specific properties whithn a certain field. Or, to put it more specifically, we can rephrase that as how site-specific yield response changes with N treatment rates conditional on other soil and field properties. -->
Idealy we wish to answer that question by applying various N rates under the completely the same condition to each site and see how yield varies by N application rates. But in reality, such an experiment is unattainable, which is know as “Fundamental Problem of Causal Inference”[@holland1986statistics].
All we can do is to <!-- just --> statistically estimate the conditional average treatment effects defined as the difference between the realized outcomes and the counterfactual potential outcomes conditional on other soil and field properties using the on-firm experiments. <!-- In other words, to estimate the ceteris paribus causal effects of treatements.  -->
Furthermore, what makes the estimation more complicated is the fact that such treatment effects are heterogenous due to the existance of factors that create within-field spatial variability[@scharf2005field; @ruffo2006site; @scharf2006spatially]. For example, higher N rete leads to greater yield at some locations, whereas yield does not respond much to change in N rate at some locations. This heterogeneity nature of the impact of N rate to yield response is what contributes the complexity of estimation of site-specific input recommendation use.
<!-- Based on the estimated unbiased treatment effects from whole field esperiments, we should determined the site-specific optimmlal N rate.\ -->
Prediction-oriented ML methods are not be able to correctly answer such a causal question. This is because prediction problem and causal inference are two different things and the algorithm of prediction-oriented ML methods are not designed for doing causal inference. Indeed, causal inference requires more of the statistical properties than prediction problem in evaluating models due to the Fundamental Problem of Causal Inference[@athey2018impact].
Hence, estimation of the heterogeneous site-specific yield responses to N should be addressed as a causal research question, not as a prediction problem. 
<!-- Indeed, the conventional predictive ML methods which are not modified for causal estimation leads to biased estimates of causal effects of interest[@athey2017beyond]. Hence, in order to deal with the challenge of VRA, we need to address it with a correct way. In this study, we want to deepen the discussion about how we can apply causal analysis in the context of VRA without giving up any merits of using ML.  -->
 
<!-- - uncertainty -->

* a movement in the academia to do more of causal machine learning. 

<!-- why people have started to realize we need causal machine learning methods for many applications. -->
<!-- Causal estimation with ML (what is its uniqueness?) -->
<!-- While the predictive performance of ML methods has gained popularity and has been widely adopted in multidicipline fields, -->
There has been a recent movement in the field of ecnomics to do causation research with ML methods[@mullainathan2017machine; @Storm2020; @Athey2019a]. This movement was establishded by the emergence of a new ML algorithm called causal forest(CF)[@Wager2018a; @athey2016recursive] which is developed to identify heterogeneous treatment effects. 
There are several reasons for this movement. Generally, ML methods have some advantages over traditional econometric methods in the process of model building. The traditional econometric methods requires reasearchers to select variables to be included in the model and to specify one functional form for modeling by themselves as well. This process makes it difficult for reasearchers to comprehensively check alternative model specifications and there is no consensus among researchers regarding on these procedures[@athey2018impact].
Meanwhile, ML methods such as RF automatically select influential variables out of many input variables and also find the best functional form out of many alternative models based on some criterion in a data-driven way. Furthermore, although the traditional econometric methods are well established about the estimation of average casual effect, they are still struggling in heterogeneous treatment effect estimation with valid asymptotic property[@knaus2018machine]. In that light, CF can do it by taking advantage of the aforementioned strengths of ML methods. 
CF is an extention of breiman's random forests, but its algolithm is distinct from prediction-oriented ML algorithms. In a pure prediction problem, the mean squate error (MSE)($\sum(y_i - \hat{y}_i)^2$) criterion is available for model validation. That is, the model that results in the smallest prediction error based on the observed outcomes is selected as an optimized model. However, when it comes to causal infernece with ML, MSE criteria is not available becasue we never observe the true causal effects of treatment. Against this problem, @Wager2018a and @athey2016recursive modified MSE criteria of regression tree algorithm and proposed a novel "honest" criterion as a solution. The regression tree adopted this honest criterion is called "Causal tree"(CT) and it provides unbiased conditional treatment effects and the valid confidence intervals under the uncounfoundedness assumption. The detail is explained in the Methodology section. Owing to this divelopment, there is a growing economic literature appliying CF for estimating treatment effect heterogeneity[@rana2019machine;@davis2017using;@o2018causal;@hitsch2018heterogeneous].
 <!-- (while conventional CART use the same sample for building a tree and estimaiting the conditional means using MSE criteria.)Then, Each node is splitted in a way that it increases the treatment effect heterogeneity. @Wager2018a extend it to causal forests(CF) which is an ensemble of CT estimators. CF enables formal statistical inference aligned with asymptotic normality theory.   -->

<!-- minimizigng the expected MSE of predicted treatment effects is equivalent to maximizing the variance of treatment effects across leaves minus a penelty for within-leaf variance -->

<!-- regularization = conlexity of a function -->
<!-- generalization using the training data -->

<!-- ?? Why other ML methods might not be appropriate for estimating treatment effect heterogeneity ?? -->

* Objective

In this study, we would like to evaluate the peformance of CF approach for site-specific input use rocommendation comparing RF and BRF which are widely accepted ML methods in the previous studies. To assure the validity and generality of our finding, we conduct 1000-round of Monte Calro simulations under varioius model configurations. Furthermore, we apply spatial smoothing with GAM to the estimated optimal N rate and examine if spatial smoothing could contribute to their accuracy improvement. 

<!-- The  general procedure has the following structure. We consider…, We… We then compare the in-sample goodness-of-fit of the model with others. …. Finally,…. -->

# Materials and Methods {-}

## Set up (Field) {-} 
The geographical data of an experimental field was created based on an actual field used in the Data-Intensive Farm Management (DIFM) project [@bullock2019]. The field consists of 1536 plots with 240 by 60 feet dementions, and the field size is about 507 acres. Each plot is subdivided into 24 subplots. In this section, we are going to explain how we simulate the field characteristics and yield data.

## Data generating process {-}
+ Yield-response function

We use Mitscherlich-Baule(MB) response functions as the underlying true yield-response function to N fertilizers for corn production. The Mitscherlich-Baule function is one of the major production functions that has been frequently investigated and applied in the literature. It features its fexibility of the shape of yield response curve <!-- allowing for both input substitution and plateau growth -->, and so it was revealed to be an plusible specification for nitrogen fertilization models for corn <!-- corn yield response to inputs -->[@frank1990; @llewelyn1997; @paris1992].
<!-- The model we are using is actually Mitscherlich model, not the Mitscherilich-Baule model? -->
In this study, we consider N as its only input. Eq.([-@eq:eqn1]) shows the equation for MB function in the study.
\begin{equation}
f(N) = ymax(1-exp(\alpha + \beta \cdot N))
\end{equation}
{#eq:eqn1}

where $ymax$ is an asymptotic yield plateau(kg), and $\alpha$ and $\beta$ ($\alpha<0, \beta<0$) are the parameters that describe soil and field properties such as inherent soil N content <!--soil-born N--> and crop N uptake <!-- soil mineralization capacity --> which characterise the process of how N fertilization affects crop yields[@llewelyn1997]. $N$ indicates applied N fertilization rate($kg \text{ } ha^{-1}$). $ymax$, $\alpha$ and $\beta$ can be regarded as uncontrollable but non-stochastic factors that affects yield response function.  Crop yield is derived as an output. $N$ is the only input variable that is controllable.
Through the study, it is assumed that the objective of a producer is to maximize profit. Then, the solution to a site-specific profit-maximizing N rate, which is optimal N rate, is calculated as the minimum N rates that can attain maximum retun to N with minimum costs as follows. 

\begin{equation}
N^{opt} =\underset{\mathrm{N}}{argMax}(P_{C} \cdot f(N) - P_N \cdot N) 
\end{equation}
{#eq:eqn2}

where $N^{opt}$ is a  site-specific profit-maximizing N rate, $P_{C}$ and $P_N$ are the price of the corn (in $/kg) and  is the price of N fertilizer(in $/ha). We assume $P_{C}=3.5kg \text{ } ha^{-1}$ and $P_N=0.6kg \text{ } ha^{-1}$.
In reality, the realized crop yield is affected by unmanageable stochastic factors such as the effect of precipitation and temperature[@bullock2002adding]. All of therse effects on yield is described by adding random yield disturbance term $u$ to the above deterministic nature of yield response function Eq.([-@eq:eqn1]). Thus, the observed crop yield is generated with the following equation.

\begin{equation}
F(N) = ymax(1-exp(\alpha + \beta \cdot N))+u
\end{equation}
{#eq:eqn3}

+ Simulated data generation

One thousand sets of parameters $ymax$, $\alpha$, $\beta$ and the random yield disturbance term $u$ are simulated in a spatially autocorrelated fashion on the basis of subplot using spherical variogram models of `R` package `gstat`[@pebesma2004; @pebesma2016]. 
One spherical variogram model is generated to simulate each of the paramers respectively. It is assumed that the range is 400m and the nugget is zero about all the spatial distributions of the parameters and random error. Regarding their sill value, $2000000 \text{ } (kg \text{ } ha^{-1})^2$ for $ymax$, $0.02$ for $\alpha$, $1$ for $\beta$ and $0.015$ for $u$ are used. $u$ approximately cause error in yields by $1300 \text{ }(kg \text{ } ha^{-1})$. After identifying each variogram model, one thousand sets of the field characteristics are respectively simulated with them using Gaussian simulation.

<!-- nism: if set to a non-zero value, conditional simulation is used instead of kriging interpolation. For this, sequential Gaussian or indicator simulation is used (depending on the value of indicators), following a single random path through the data. -->



<!-- How should I explain about sp_range and so on?   -->
Figure \@ref(fig:fig-parameters) shows an example of distribution maps of $ymax$, $\alpha$, $\beta$ and $u$. The spatial distribution of $ymax$, $\alpha$ and $\beta$ are assumed to be independent each other. $ymax$ and $\alpha$ are also assumed to be independent from the random yield disturbance term $u$, but we impose an assumption on $\beta$ that it is correlated with $u$ to some extent. This assumption is reasonable to represent realistic crop production system. Soil and field properties are often affected by stochastic facotrs. For example, weather conditions such as temperatures not only have an impact on nitrogen mineralization which is the major soil N dynamics[@clark2020soil] but it also affect final crop yield through heat stress[@siebert2014impact]. In this case, temperatures not only affect crop yield directly but also have some impact on yield indirectly through affecting soil N properties which is represented by $\beta$. Once $ymax$, $\alpha$ and $\beta$ are generated, optimal N rate are calculated site-specifically by Eq.([-@eq:eqn2]). Figure \@ref(fig:fig-optN) shows an example of the distribution map of true optimal N application rate derived from the distribution of Figure \@ref(fig:fig-parameters), idicating spatially autocorrelated pattern of the optimal N in the field.
<!-- #' 0.015 means 1300 sd, -->

## Experimental Design
In each simulation round, five N fertilizer treatment rates are randomly assinged to each of the field's 1536 plots following a Latin square design. Figure \@ref(fig:fig-nExp) shows an example of experimental design. Regarding the determination of the N treatment rates to be used for each simulation round, first, minimum and maximum N treatment rates are determined so that they represent extreamly low and high N application rates respectively based on the probability density of true optimal N rates for that simulation round. Then, the rest of the three N treatments rates are obtained by finding N rates which equally partitioning the range between minimum and maximum N treatment rates into three parts. 
<!-- you might wanna explain how to calculate optimal N by showing the density plot of optimal N-->
In Figure \@ref(fig:fig-nExp), the five N treatment rates are set to be 81, 116, 150, 184 and 219 kg/ha. According to those assined N treatment rates, subplot-basis yield data is generated with Eq.([-@eq:eqn3]). Figure \@ref(fig:fig-yield) shows an example of a yeild distribution map.
The simulated data including yield and the parameters of the response functions is aggreagated by taking mean value by plot and the resulting data is used for training data. 
For testing data, due to the computational burden accompanied by Monte Carlo simulations, we use randomly selected 1000 subplots out of the simulated data different from the training data. <!-- We use this subplot-basis data as a testing data. -->

<!-- + a map of an experimental design  -->

## Methods (Causal Forest, Random Forest, and Boosted Random Forest)
We evalauate three ML methods; RF, BRF, and CF usign the simulated data explained in the above. We use R-squared($R^2$) statistic as a metric for the performance evaluation. $R^2$ is derived from a simple linear regression model where true optimal N rates are regressed on estimated optimal N rates in each simulation round.
After that, for the purpose of spatially smoothing, we fit GAM to the resulted estimates, then re-estimate site-specific optimal N rate. $R^2$ is again calculated using the re-estimated optimal N rates to see how spatial smoothing works for prediction improvement. In this section, we are going to look at the details of each of the ML methods and GAM and explain how we implement them for our analysis. 

+ RF

RF is a collection of multiple deep regression trees. Individual tree is built by recursively splitting samples into two disjoint subgroups<!-- , which is called "leaves," -->  based on the covariates<!-- feature space -->. At each stage, average of response is derived as a predicted value in each group and it is evaluated to find best splitting variable and split point. The problem of a regression tree is that it is easily overfit the data, leading to high variance. This problem gets worse as a tree grows more complecatedly[@friedman2001elements; @efron2016computer]. In random forest, each tree is generated from bootstrap and the splits are determined by a random subset of the explanatory vairables at each stage. By growing many different trees and averaging the outcomes from all the trees, the generalization error converges following the Law of Large Numbers[@breiman2001random]. Hence, RF can effectively reduce the variance and attain more precise prediction comapared to any single tree. RF is robust to irerelevant variables to the responses and does not require much about its tuning[@Storm2020; @Athey2019a]. 
RF is one of the popular ML methods in the literature of precision agriculture and many studies addmitted their performance as a tool for yield prediction and as a site-specific N recommendation tool as well[@zha2020; @lawes2019; @Ransom2019; @everingham2016; @Wang2021]. 


+ BRF

BRF is similar to RF in the sense that their prediction is derived from the average of outcomes from individual tree in the forest. The major difference lies in their ways of building tree. While trees of RF is grown applying the same process incorporating some randomness to each building stage, trees of BRF are sequentialy grow modifying the tree based the error from the previous stage. 
<!-- find the papers that use gradiant boosting method or support the method  -->
@Ghosal2020a proposed one-step boosted forests by taking gradient boosting method(@friedman2001greedy) as a part of random forest procedure of @Wager2018a to improve its prediction. 
In one-step boosted forests, two forests are build; first one is the usual random forest build on subsamples of the training set and the second is constructed using the out-of-bag<!-- (OOB) --> residuals from the first random forests. The final output is derived as the sum of the two outputs from each if the random forests. @Ghosal2020a showed that one-step random forest outperformes usual random forest and gradiant boosting for their dataset. 

<!-- In one-step boosted forests, given the current output from the tree, the loss function is calculated and  -->
<!-- @Ghosal2020a: it can be expected that more boosting stpes, instead of just one step, lead to more improvement. But the development of stopping criteria test is left to future work-->

+ CF 

As aforementioned, CF is an ensemble of CT estimators and it estimates conditional average treatment effect.<!--  defined as $\hat{\tau}(x) = E[Y_1 - Y_0|X=x_i]$. Let $\mathbf{c_{i}}$ be a vector of soil and field properties of subplot $i$ and let $Y_1$ and $Y_0$ be the yield reponse with and without treatment. --> The same idea of RF is applied about taking ensemble form of multiple trees. By doing so, it works for smoothing the outputs and reducing variance. While a standard regression tree splits the samples into smaller subgroups based on the covariates, CT does that based on the treatment effect heterogeneity. 
More specifically, at each building stage of a CT, splitting variable and splitting point are determined so that the variance of treatment effects <!--  $\hat{\tau}(x)$  --> are maximized across leaves. Then, within-leaf estimates are derived as an average of the estimates of the treatment effects of the samples which are fallen in the same leaf. For these split selection and leafwise estimation, separated subsamples are used(honesty condition)[@Wager2018a; @athey2016recursive]. @Wager2018a mentions that the predictions of CF are not only an unbiased and Gaussian but also it is better than standard RF with respect to mean-square error. 
The most fundamental underlying assumtion of CT as well as CF is unconfoundedness. This assumption means that potential outcomes and treatment assignment are independent. In the study, this assumption is justified by simulating data so as to treatment assignment is completely randomized.

+ detailed explanations of how you implement them 

For implementation of RF, BRF and CF, `R` package `grf`, version 1.2.0 [@tibshirani2018package] is used. In `grf` package, RF and BRF are built with the same goodness-of-fit criteria as the standard RF, which is MSE criteria. The distinct points are that each tree uses subsamples of the training data instead of bootstrap samples, and honesty condition is applied. In regards to BRF of `grf`, the boosting method of @Ghosal2020a is reatedly applied to RF of `grf`, and the number of boosting iterations is automatically selected based on corss-validation.\


+ Profit-maximizing Site-specific N rate 

For RF and BRF, finding profit-maximizing site-specific N rate is straightforward. RF and BRF site-specifically identify a single yield response function given soil and field properties for that location. Let $\hat{y}_{i}(N) = \hat{g}(N,\mathbf{c_{i}})$ be an estimated yield response function to N, where $\hat{g}(\cdot)$ is the resulted model of RF or BRF, $N$ is applied nitrogen rate and $\mathbf{c_{i}}$ indicates the soil and field properties forl location $i$. Then, the profit for this location is expressed as $\pi_{i}(N) = P_{C} \cdot \hat{y}_{i}(N) - P_N \cdot N$. The optimal N rate is estimated by chaging $N$ sequentially and finding the $N$ that maximizes the profit. This process is applied to all the subplots of testing data every simulation round.\
Now consider the case of CF. Bear in mind that the goal of CF approach is to identify heterogeneous "binary" treatment effects. Since we have five N treatment rates, four CFs are built using four pairwise combinations of N treatment rates combinations respectively. Let $N_1,N_2,N_3,N_4,N_5$ be the five N treatment rates to be applied and suppose that $N_1<N_2<...<N_5$. We consider two possible approaches to make such pairwise combinations. \
First approach is the way of grouping as follows; $(N_1, N_2),(N_2, N_3),(N_3, N_4),(N_4, N_5)$. We refer to this approach as "stepwise". In this case, the estimated treatment effects for location $i$ from CFs with these pairwise combinations are discribed as
$\hat{\tau}_{N_{k} \rightarrow N_{k+1}}(\mathbf{c_{i}}) = E[Y_i(N_{k+1}) - Y_i(N_{k})|\mathbf{c_{i}}]$,
where $k\in\{1,2,3,4\}$ and $Y_i(N_{k+1})$ and $Y_i(N_{k})$ are potential yield outcomes corresponding to the two possible N treatment rates. 
Once these treatment effects are estimated, we predict change in yield due to the increase in applied $N$ rate relative to the yeild of $N_1$. It is calculated by sequentially adding the estimated treatment effects. 
Generally, the change in yield going from $N_1$ to $N_m$ (i.e. $m\in\{2,3,4,5\}$) is generally described as 
$\Delta Y_{N_1 \rightarrow N_m}(\mathbf{c_{i}}) = \sum_{k=1}^{m-1} \hat{\tau}_{N_{k} \rightarrow N_{k+1}}(\mathbf{c_{i}})$.
<!-- Thus the corresponding change in profits is $\Delta \pi_{N_1 \rightarrow N_m}(\mathbf{c_{i}})=P_c \cdot\Delta Y_{N_1 \rightarrow N_m}(\mathbf{c_{i}}) - P_N \cdot (N_m - N_1)$. -->
<!-- The profit maximizing site-specific optimal N rate can be identified by finding $N_m$ that maximizes $\Delta \pi_{N_1 \rightarrow N_m}(\mathbf{c_{i}})$. \ -->
Second approach is the way of grouping as follows:$(N_1, N_2),(N_1, N_3),(N_1, N_3),(N_1, N_5)$. We call this approach as "base". In base approach, treatment effect is always estimated in the comparison with the least treatment rate $N_1$, and therefore it already represents the change in yield relative to the yield of $N_1$. 
That is, 
$\Delta Y_{N_1 \rightarrow N_m}(\mathbf{c_{i}}) = \hat{\tau}_{N_{1} \rightarrow N_{m}}(c_{i})=E[Y_i(N_{m}) - Y_i(N_{1})|\mathbf{c_{i}}]$.
After $\Delta Y_{N_1 \rightarrow N_m}(\mathbf{c_{i}})$ is obtained in both approach, the corresponding change in profit is derived as 
$\Delta \pi_{N_1 \rightarrow N_m}(\mathbf{c_{i}})=P_c \cdot\Delta Y_{N_1 \rightarrow N_m}(\mathbf{c_{i}}) - P_N \cdot (N_m - N_1)$.
Then, the profit maximizing site-specific optimal N rate can be identified respectively by finding $N_m$ which leads to maximum change in profit defined as $\Delta \pi_{N_1 \rightarrow N_m}(\mathbf{c_{i}})$.



+ spatial smoothing <!-- (or spatial interpolation, generalize the trend) -->
<!-- ML cannot account for spatial information, and therefore the resulting optimal N rates lacks the spatially smooth distribution (read @kasampalis2018contribution)  -->

From agronomical and geographical perspectives, it is well-know fact that any field characteristic shows similaliry in its neighborhood and it varies smoothly as goes spatially further[@f2007methods;@goovaerts1999geostatistics]. The saptial distribution of optimal N rates is not an exeption becuase it is determined by varioius soil and field characteristics which have spatially smooth sturucture. However, ML methods cannot account for such a structure, and therefore the resulting estimates of optimal N rate fail to explain its saptially continuous feature. Hence, we impose spatial smoothness to the estimates of optimal N rate. Especially, we expect that spatial smoothing is greatly helpful for the estimated optimal N rates of CF to improve its prediction accuracy of recommendation N rates because, as described in the above, the valiability of the optimal N rate from CF approach can represent is limited and those values are highly discrete. We expect that generalization of the outcomes of CF could represent more realistic and accurate results.\
Considering the fact that site-specific optimal N rates depend on other field and soil properties, it is desirable to generalize the results taking such an complex relationship into consideration. For this reasons, we use generalized additive models (GAM) [@hastie1990generalized] for spatial smoothing which is the extension of generalised linear models(GLM). <!-- A generalized additive model (GAM) is a generalized linear model (GLM) in which the linear predictor is given by a user specified sum of smooth functions of the covariates plus a conventional parametric component of the linear predictor -->Compared to GLM, GAM can model the relationship between dependent variable and covariates in more flexible way as it nonparametrically identifies the functional form for each covariate, which is called as smooth function, in a data-driven manner. Thus, GAM perfomes better at identifing nonlinear effect of covariates on dependent vairable[@friedman2001elements; @yee1991generalized]. 
As covariates for GAM estimation, we include each subplot's coordinates$(X,Y)$, which are projected by CRS: WGS 84 / UTM zone 16N, and their interaction term as well as the each of the covariates used in estimating optimal N rates with ML. The smooth function related to geographical coordinates is expected to capture the spatial trend of the estimates of optimal N rate. Using the esitmated GAM, we re-estimate site-specific optimal N rates. \
For the estimates of optimal N rates(e.g. $\hat{N}^{opt}$) that are derived with a specific model configuration(described in the next section) with $p$ covariates(e.g. $c_1$, $c_2$,...$c_p$), the GAM is fitted with the following form:
$$
\hat{N}^{opt} = s_0 + s_1(c_1) + s_2(c_2) + ... + s_p(c_p)+ + s_{p+1}(X) + s_{p+2}(Y) + s_{p+3}(X \cdot Y)
$$
{#eq:eqn4}
where $s_1(\cdot), s_2(\cdot)$,..., $s_{p+3}(\cdot)$ are smooth functions and each of them represens the impact on the estimated optimal N rate. GAM is implemented with `R` package `mgcv` version 1.8-34[@wood2011fast; @wood2016smoothing; @wood2004stable; @wood2017generalized; @wood2003thin]. In the package `mgcv`, we use thin plate splines with five dimensions of the basis to estimate each of the smooth terms.



+ variations in parameters (you should check this section more carefully) 

We add some variations in our covariates to be used in the regression of RF, BRF and CF. This is so as to make our simulation more plausible and to check the robustness of the performance of each of the ML methods.
In practice, we never know which variables have more explanatory power to the realized yield valiability. Furthermore, the relative importance of variables are varies by geographical location of fields[@ge2011remote]. The past studies deal with such a situation by including many variables into models. Some of those variables are highly correlated with other varibales[@Ransom2019]. We reproduce it by incorporating additional covariates(e.g. $\theta_1$,$\theta_2$).
$\theta_1$ and $\theta_2$ are generated in a way that they are spatially autocorrelated respectively and they are irerelevant to yield response function but are somewhat correlated with a factor that is influential to yield. Here, we assume that $\theta_1$ and $\theta_2$ are correlated with $\beta$.\
Furthermore, it may be unrealistic to assume that each of the parameters of the "true" yield function($ymax$, $\alpha$, $\beta$) is directly measurable. In practice, more accessible <!-- variables or  -->measurments which are highly correlated with the target variable are alternatively used when target variable are difficult to measure[@puntel2019development]. In our case, for example, $ymax$ which indicate potential yield is not directly measurable. Instead of using $ymax$, some topographical land features and soil properties such as elevation and soil sand fraction could be used as yield-limitting factors[@jiang2004effect; @kravchenko2000correlation]. Following this way, we splitted each of the variables $ymax$, $\alpha$ and $\beta$ into two variables respectively(e.g. $\alpha_1$, $\alpha_2$, $\beta_1$, $\beta_2$, $ymax_1$, $ymax_2$). Each of the splitted variables are generated by multiplying a spatially autocorrelated split ratio to the original variables, and therefore, they also show spatially autocorrelated pattern.\
Using these variables, we created four sets of covariate groups. Each of them represents different assumptions about how much amount of information about yield response function the farmer has and whether target varibales that consist of the yield response function(e.g. $\alpha$, $\beta$, $ymax$) are directly measurable. \
<!-- It might be more understandable to explain using table -->
1. "aby" includes $\alpha$, $\beta$ and $ymax$; This refelcts the situation where the farmer knows the factors that affects yield and such factors are directly measurable \
2. "abytt" includes $\alpha$, $\beta$, $ymax$, $\theta_1$, $\theta_2$; This refelcts the situation where the farmer does not know about which factors affects yield but the factors that truely affects yield are directly measurable\
3. "aabbyy" includes $\alpha_1$, $\alpha_2$, $\beta_1$, $\beta_2$, $ymax_1$ and $ymax_2$; This refelcts the situation where the farmer knows the factors that affects yield but such factors are not directly measurable\
4. "aabbyytt" includes $\alpha_1$, $\alpha_2$, $\beta_1$, $\beta_2$, $ymax_1$ $ymax_2$, $\theta_1$, $\theta_2$; This refelcts the situation where the farmer does not know which factors affects yield and the factors that truely affects yield are also not directly measurable\
These four covariate groups are used in each ML methods. Hence, in each simulation round, sixteen model configurations(4 ML methods(RF, BRF, CF_stepwise, CF_base) $\times$ 4 covariate groups) are evaluated.


# Results and Discussions {-}

<!-- in that it can capture the valiability of site-specific optimal N rate for the field by correctly estimating causal effect of nitrogen to yield -->
Table \@ref(tab:table-simSummary) shows the summarised results of 1000 MonteCalro simulation rounds.<!--  Model evaluation was conducted based on avaraged $R^2$ values for both optimal N recommendation rates from ML methods and spatial smoothing by GAM. --> Across all the models, CF_base method consistently performed the best at capturing the variability of true optimal N rates, whereas the performance of RF was the worst. Including irrelevant varibles did not affect the perfomance of each ML methods seriously, indicating that all the ML methods successfully distinguish the important varibales for modeling the effect of N to yield. 
The differences in performance of each method are clrear in looking at Figure \@ref(fig:fig-distRes). Figure \@ref(fig:fig-distRes) shows the distributions of $R^2$ values derived from the estimates of ML methods. Most of the time, CF_base consistently produces much more accurate estimates of optimal N rate. 
Although both CF_stepwise and CF_base were based on the same ML method, the results of CF_stepwise are largly different from CF_base. On the contrary, the performace of CF_stepwise was not different from BRF. As explained in the previous section, CF_stepwise esitmates treatement effects of N rate relative to the next lower N rate. In this sense, we expected that CF_stepwise is relatively better at sensitively capturing heterogeneous change in yield accompanied by an additional N application. However, in finding profit-maximizing optimal N recommendation rate, the estimated treatment effects are added sequentially asuuming the linear relationship between each treatment effects and yield difference. 
Although CF can estimate unbised treatment effect, considering the fact that the each estimated treatment effects contain more or less uncertainty or noise originated from unobservable random error and model identification, taking sum of those estimated treatment effects is likely to inflate such uncertainty, resulting in over or underestimation of "true" incaseses in yield casued by an additional N application. Meanwhile, CF_base estimates the treatement effects of each N rates relative to $N_1$, and therfore it derives change in yield relative to the yield of $N_1$ directly. Hence, CF_base is less likely to suffer from such an error compared to CF_stepwise. \
Spatial smoothing considerably improved the accuracy of optimal N recommendation rates across all the sinarios.
Figure \@ref(fig:fig-distGamRes) shows the distributions of $R^2$ values of re-estimated optimal N rate by GAM. Compared to Figure \@ref(fig:fig-distRes), we can see the obvious differences in the shape of their distributions. Although the distributions of $R^2$ of RF are still almost flatten-out, the distributions of the other methods indicate that their estimates becomes to attain more precise and consistent for all the simulation rounds. The average improvement of $R^2$ values after Spatial smoothness are $0.153$ for RF, $0.185$ for BRF, $0.134$ for CF_stepwise and $0.127$ for CF_base. The degree of improvement of CF is smaller than for RF and BRF. This is because RF and BRF can estimate continuous yield response function and thus the resulting oprimal N recommendation rate are continuous values. They can easily be transformed to spatially smooth. Whereas, the values CF can pick up as optimal N rate are limited and discrete. Due to this restriction, it is hard for GAM to make generalize the results of CF. Still, spatially smoothed CF_base approach perfomed better in estimating optimal N rate than RF and BRF overall. It is noteworthy that $R^2$ value of CF_base under "aabbyytt" modeling sinario is $0.683$, which is comparable with the BRF $R^2$ value of $0.701$ under "aby" modeling sinario. This suggest that even though there exists high uncertainty in estimating underlying "true" yield response function, CF_base with spatially smoothing can achive reasonably robust estimatation for site-specific optimal N rate.\
<!-- Another important thing is to be noted is that even thoguth "aby" modelinng sinario reflects no uncertaity in 
indicating that spatial smoothing is helpful for explaining unobservable spatila structure  -->
<!-- try to exlain the behind mechanism of CF_base and why its performance is the best  -->
In summary, CF_base approach is generally superior to RF and BRF which are currentry widely used for site-specific nitrogen recommendation tool, and CF_base with spatial smoothing are robust even though we do not have sufficient knowledge about underlying "true" yield response function for a field, which is commmon case in practice.


# Conclusion {-}
<!-- - We showed that CF_base approach which forcus on heterogeneous treatment effects of N on yield response provides more precise N recommendation rate than RF and BRF.  -->
PA is a promissing technolody for modern agricultural production system to grasp spatial and temporal within-field heterogeneity. On-farm experiments incorporating PA tehcnology enables reaserchers and farmers to collect more spatially dense data related to the underlying heterogeneous yield response function <!-- site-specific field information --> and to conduct finer scale of an input management. Given the fact that an input for a biological crop systems are are higly complicated and involves numerous factors, ML methods is appealing as a tool to describe such an complex relationship and to derive the useful inforamtion for within-field site-specific input application. In this study, we considered N fertilizer as an input to be optimazed because N is an essential nutriant for crop growth and its usage has profound impact on both economics and environment. <!-- Past studies used prediction based ML methods represented by RF and BRF for N recommendation tools in an attempt to find the best prediction model that can account for the valiability of true optimal N. However, what we need to deal with in order to know site-specific optiomal N rate is to estimate the heterogeneous causal effect of a treatemtment N rate to yield. Prediction and causal inference are two different things and prediction itself cannot tell correct information about that. --> <!-- Based on the estimated treatement effects, we should make a dicision about how much amount of N fertilizer should be applied to each site.  -->
Acknowledging that site-specific optimal N recommendation should be based on a causal question, in this study, we proposed an approach using CF[@Wager2018a; @athey2016recursive] as the recommendation tool.<!-- CF is developed to estimate heterogeous treatment effects. --> We devised two approaches using CF; CF_stepwise and CF_base, and tested their performances as N rate recommendation tool comparing RF and BRF with 1000 rounds of Monte Calro simulations. The results suggested that CF_base approach shows the best at capturing the valiability of true optimal N rates compared to RF and BRF under all the model configurations. On the other hand, the performance of CF_stepwise did not change much from BRF overall. CF-stepwise is highly likely to suffer from the bias when predicting change in yield relative to the yield of the lowest N application rate, leading to inaccurate estimates of optimal N rate. 
Another our significant finding is that spatially generalizing the estimated optimal N recommendation rates results in considerable improvement in prediction accuracy of optimal N rate estimation. We applied GAM to the resulted optimal N rates incorporing geographical coordinates and soil and field properties as well. We confimed that spatial smoothing led to great improvement in $R^2$ of all the model configurations.\
Based on these findings, we concluded that estimation of causal impact of an input on yield is crucial for site-specific input recommendation. CF is a promising tool to do that without giveing up any strengths of ML methods that past studies put an values on. Especially, we found that the combination of CF_base and spatial smoothing is more reliable tool for site-specific input management tool compared to RF, BRF. Although we only considered a single input to be optimized for farm management, our methods can be expanded for multivariate input optimization.

# Limitations {-}
<!-- temporal variability may change the causal impact of inputs on yield if we didn't consider it-->
Even though our finding shows the comparative advantages of CF over RF and BRF, there is still much room to consider more in detial in applying CF-based approach in practice. 
MB function which we emploied as an underlying true yield response function to N may not enough to represent the complexity nature of the relationship between N and yield in the real corn production system. 
Especially, it fail to represent the temporal variation of optimal N rates. Site-specific optimal N rates vary annually partly due to the carryover effect of an N fertilizer and crop-management practices applied in the previous season on soil N in the next season[@puntel2018systems].<!-- cite more papers and talk about weather uncertainty--> Although consideration of such temporal variations in optimal N also important for site-specific N management[@mamo2003spatial;@dhital2016variability], simulated yield data with MB function in this study cannot tell those temporal variations becyase every simulation is conducted independently. It would be better if we could generate each season's yield data sequentially so that previous season's management dicision affect the next season's soil properties. Previous studies adopted crop simulation models such as Agricultural Production Systems sIMulator model to represent dynamic nature of cropping system in the context of nitrogen management[@archontoulis2020predicting;@puntel2018systems;@puntel2016modeling]. Therefore, it will be fruitful future work to explor the performance of CF-based approach based on more realistic data generated with those simulation models. 




<!-- + We dismiss the dynamics nature of optimal N rates.  (solution: APSIM model[@puntel2018systems;@archontoulis2020predicting])
• our model(MB function) cannot fully account for dynamic properties of optimal N rates.
•  
• temporal variability
• "carryover effects on soil N, water, residue, and soil organic matter dynamics from 1 year to the next"
• Although past studies shows that such an complicated models did not necessarily result in the improvement in site-specific optimal N rate estimates, it is not still tested using CF.  

+ We dismiss some other important facotrs related to management and environmental variables
• agronomic practices ('management operations(such as tillage, seeding, weed and pest management, irrigation, and harvest)' or crop-management practices)
• soil-plant-atomosphere system and uncertainty in weather[@puntel2016modeling]


+ we need to address the ex-ante
•  -->

<!-- 
Furthermore, we did not consider an input application error in our simulation. However, in practice, an input application error is highly likely to occure because machinery may fail to apply the accurate rate of an input simply due to the machinery error or topographical feature of the field. We need to address how this application error affect the performance of CF. Similary, measurement error could affect the performance of GAM in the from of attenuation bias.  -->





<!-- 1. yearly change -->
<!-- site-specific optimal N rate changes by yearly due to
  -->




<!-- "further advancement of crop simulators is crucial for validation and improvement of the current algolithm" -->
<!-- we consider  -->




# Acknowledgement {-}
This research was supported by a USDA-NIFA-AFRI Food Security Program Coordinated Agricultural Project, titled “Using Precision Technology in On-farm Field Trials to Enable Data-Intensive Fertilizer Management,” (Accession Number 2016-68004-24769), and also by the a USDA-NRCS Conservation Innovation Grant from the On-farm Trials Program, titled “Improving the Economic and Ecological Sustainability of US Crop Production through On-Farm Precision Experimentation” (Award Number NR213A7500013G021). 



\newpage

# Tables 
```{r table-simSummary, tab.cap = "Summary of the 1000 Monte Calro simulation's results"}
table_res400_summary
```


<!-- ```{r table-2, tab.cap = "Results of model evaluation(sp_range=600)"}
table_res600
``` -->
\newpage


# Figures
<!-- 
1. add the distribution of the resutls
2. add the distribution of each parameters -->
<!-- when you mention a figure, use "\@ref(fig:fig-1)"  -->
```{r fig-parameters, fig.width=9, fig.height=10, fig.cap = "An Example of a Distribution Maps of Field Charateristics"}
# field_alpha + field_beta + field_ymax
grid.arrange(field_ymax, field_alpha, field_beta, field_m_error, ncol=2,nrow=2)
```

\newpage


```{r fig-optN,  fig.cap = "An Example of a Distribution Map of True Optimal Nitrogen Rate"}
field_optN
```

<!-- \newpage -->

<!-- ```{r fig-error,  fig.cap = "An Example of a Distribution Map of random yield disturbance"}
field_m_error
``` -->

```{r fig-nExp, fig.cap = "An Example of Experimental Design of Nitrogen"}
field_Ndesign
```

```{r fig-yield, fig.cap = "An Example of a Yield Distribution Map"}
field_yield
```


```{r fig-distRes, fig.width=7, fig.height=10, fig.cap = "Distributions of R squared values from 1000 Monte Calro Simulations before GAM is Applied"}
dist_case
```

```{r fig-distGamRes, fig.width=7, fig.height=10, fig.cap = "Distributions of R squared values from 1000 Monte Calro Simulations after GAM is Applied"}
dist_case_gam
```

# References

<div id="refs"></div>












